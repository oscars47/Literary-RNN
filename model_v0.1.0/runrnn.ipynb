{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 00:59:47.575192: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 00:59:47.737372: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-12 00:59:47.759039: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:47.759048: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-12 00:59:48.219549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:48.219580: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:48.219584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 00:59:49.217293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:59:49.217842: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:49.217874: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:49.217892: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:49.217912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:49.217929: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:49.217954: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:49.217979: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:49.218003: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-12 00:59:49.218008: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'é', 'ê', 'ï', '—', '‘', '’', '“', '”']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moscarscholin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oscar47/Desktop/thinking_parrot/Literary-RNN/model_v0.1.0/wandb/run-20221212_005953-3k7afv5m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/3k7afv5m\" target=\"_blank\">worldly-dawn-1</a></strong> to <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Additional properties are not allowed ('goal' was unexpected)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. 'val_loss' is not of type 'object'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ujx0v91l\n",
      "Sweep URL: https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/ujx0v91l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: Synced worldly-dawn-1: https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/3k7afv5m\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20221212_005953-3k7afv5m/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uylwvpib with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_1: 149\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_2: 139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_3: 122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_4: 204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_5: 117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.13231846471139588\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08672308223686026\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oscar47/Desktop/thinking_parrot/Literary-RNN/model_v0.1.0/wandb/run-20221212_010003-uylwvpib</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/uylwvpib\" target=\"_blank\">toasty-sweep-1</a></strong> to <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/ujx0v91l\" target=\"_blank\">https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/ujx0v91l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 01:00:05.856545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 233/4603 [>.............................] - ETA: 54:16 - loss: 5.4223"
     ]
    }
   ],
   "source": [
    "# file to train network\n",
    "# @oscars47\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import *\n",
    "\n",
    "# check GPU num\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from dataprep2 import TextData # import TextData class for processing\n",
    "from modelpredict2 import * # get functions to interpret output\n",
    "\n",
    "# define path\n",
    "MAIN_DIR = '/home/oscar47/Desktop/thinking_parrot'\n",
    "DATA_DIR = os.path.join(MAIN_DIR, 'texts_prep') # main\n",
    "# DATA_DIR = os.path.join(MAIN_DIR, 'texts_prep', 'test') # for testing\n",
    "\n",
    "# define master txt file\n",
    "MASTER_TEXT_PATH = os.path.join(MAIN_DIR, 'texts', 'master.txt')\n",
    "#MASTER_TEXT_PATH = os.path.join(MAIN_DIR, 'texts', 'toaster_man.txt')\n",
    "\n",
    "# initialize text object\n",
    "maxChar = 100\n",
    "master=TextData(MASTER_TEXT_PATH, maxChar)\n",
    "# get alphabet\n",
    "alphabet = master.alphabet\n",
    "char_to_int= master.char_to_int\n",
    "int_to_char = master.int_to_char\n",
    "text = master.text\n",
    "\n",
    "# read in files for training\n",
    "x_train = np.load(os.path.join(DATA_DIR, 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(DATA_DIR, 'y_train.npy'))\n",
    "x_val = np.load(os.path.join(DATA_DIR, 'x_val.npy'))\n",
    "y_val = np.load(os.path.join(DATA_DIR, 'y_val.npy'))\n",
    "\n",
    "# build model functions--------------------------------\n",
    "def build_model(LSTM_layer_size_1,  LSTM_layer_size_2, LSTM_layer_size_3, \n",
    "          LSTM_layer_size_4, LSTM_layer_size_5, \n",
    "          dropout, learning_rate):\n",
    "    # call initialize function\n",
    "    \n",
    "    model = Sequential()\n",
    "    # RNN layers for language processing\n",
    "    model.add(LSTM(LSTM_layer_size_1, input_shape = (2*maxChar, len(alphabet)), return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_2, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_3, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_4, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_5))\n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(len(alphabet)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "    # put structure together\n",
    "    optimizer = RMSprop(learning_rate = learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "    # If called by wandb.agent, as below,\n",
    "    # this config will be set by Sweep Controller\n",
    "      config = wandb.config\n",
    "\n",
    "      #pprint.pprint(config)\n",
    "\n",
    "      #initialize the neural net; \n",
    "      global model\n",
    "      model = build_model(config.LSTM_layer_size_1,  config.LSTM_layer_size_2, config.LSTM_layer_size_3, \n",
    "              config.LSTM_layer_size_4, config.LSTM_layer_size_5, \n",
    "              config.dropout, config.learning_rate)\n",
    "      \n",
    "      #now run training\n",
    "      history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = config.batch_size,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=config.epochs,\n",
    "        callbacks=callbacks #use callbacks to have w&b log stats; will automatically save best model                     \n",
    "      ) \n",
    "\n",
    "def train_custom(LSTM_layer_size_1=128,  LSTM_layer_size_2=128, LSTM_layer_size_3=128, \n",
    "              LSTM_layer_size_4=128, LSTM_layer_size_5=128, \n",
    "              dropout=0.1, learning_rate=0.01, epochs=1, batchsize=32):\n",
    "    #initialize the neural net; \n",
    "    global model\n",
    "    model = build_model(LSTM_layer_size_1,  LSTM_layer_size_2, LSTM_layer_size_3, \n",
    "            LSTM_layer_size_4, LSTM_layer_size_5, \n",
    "            dropout, learning_rate)\n",
    "    \n",
    "    #now run training\n",
    "    history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = batchsize,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks #use callbacks to have w&b log stats; will automatically save best model                     \n",
    "    ) \n",
    "\n",
    "\n",
    "# helper functions from Keras\n",
    "\n",
    "def get_toast_len(mean, stdev):\n",
    "    toast_len = int(np.random.normal(mean, stdev))\n",
    "    return toast_len\n",
    "\n",
    "# do this each time we begin a new epoch    \n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "\n",
    "    for diversity in [0.1, 0.5,1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        start_index = np.random.randint(0, len(text) - maxChar - 1) +1\n",
    "        # need to check how much to pad\n",
    "        if start_index < maxChar:\n",
    "            sentence0 = text[0:start_index] # up to but not including start index\n",
    "            sentence1 = text[start_index+1: start_index+start_index+1]\n",
    "            sentence = sentence0+sentence1\n",
    "        else:\n",
    "            stdev = (1/2)*(maxChar - 1)\n",
    "            mean = (maxChar - 1)\n",
    "                 # compute len, following normal distribution between 1 and maxChar; will go from [:num] as first part then [num+1:] concatenated; predict at num\n",
    "                # need toastlen positive but no more than  maxChar\n",
    "            goodtoast = False\n",
    "            while goodtoast==False:\n",
    "                toast_len = get_toast_len(mean, stdev)\n",
    "                # add 1 to len since the distr here goes from 0 up to maxChar-1\n",
    "                toast_len+=1\n",
    "                #print(toast_len)\n",
    "                if (toast_len > 0) and (toast_len <= maxChar): # if get acceptable toast, can leave\n",
    "                    goodtoast=True\n",
    "                    break\n",
    "            sentence0 = text[start_index-toast_len:start_index]\n",
    "            sentence1 = text[start_index+1: start_index+toast_len]\n",
    "            sentence =  sentence0+ sentence1\n",
    "        \n",
    "        # need another condition here about if neat the end\n",
    "\n",
    "        # 1. compute difference from maxChar and len/2\n",
    "        diff = maxChar - int(len(sentence)/2)\n",
    "        # need to check even/odd so we don;t overcount\n",
    "        if len(sentence) %2 != 0: # if odd: need to subtract 1\n",
    "            diff-=1\n",
    "\n",
    "        # 2. initialize new string for each sentence\n",
    "        complete_sentence = ''\n",
    "        for i in range(diff):\n",
    "            complete_sentence+='£' # appending forbidden\n",
    "        # 3. now add 'real' sentence\n",
    "        complete_sentence+=sentence\n",
    "        # 4. append forbidden again\n",
    "        for i in range(diff):\n",
    "            complete_sentence+='£'\n",
    "\n",
    "        print(len(complete_sentence))\n",
    "        print(len(sentence))\n",
    "\n",
    "        generated = ''\n",
    "        #generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        #sys.stdout.write(generated)\n",
    "\n",
    "        # generate 400 characters worth of test\n",
    "        for i in range(400):\n",
    "            # prepare chosen sentence as part of new dataset\n",
    "            x_pred = np.zeros((1, 2*maxChar, len(alphabet)))\n",
    "            #x_pred = np.zeros((2*maxChar, len(alphabet)))\n",
    "            for t, char in enumerate(complete_sentence):\n",
    "                if char != '£': # encode 1 iff it's not padded\n",
    "                    x_pred[0, t, char_to_int[char]] = 1.\n",
    "                    #x_pred[t, char_to_int[char]] = 1.\n",
    "\n",
    "            # use the current model to predict what outputs are\n",
    "            preds = model.predict(x_pred, verbose=0)[0] # removed [0] here\n",
    "            # call the function above to interpret the probabilities and add a degree of freedom\n",
    "            next_index = sample(preds, diversity)\n",
    "            #convert predicted number to character\n",
    "            next_char = int_to_char[next_index]\n",
    "\n",
    "            generated+=next_char\n",
    "\n",
    "            # check size of sentence; if still small can keep old stuff in sentence0\n",
    "            if len(sentence) >= 2*maxChar:\n",
    "                sentence0 = sentence0[1:]\n",
    "            sentence0 += next_char # append new middle character\n",
    "            sentence=sentence0+sentence1 # append to main sentence\n",
    "\n",
    "            # print the new character as we create it\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "# define search parameters-----------------\n",
    "# holds wandb config nested dictionaries\n",
    "# @oscars47\n",
    "\n",
    "# set dictionary with random search; optimizing val_loss\n",
    "sweep_config= {\n",
    "    'method': 'random',\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'\n",
    "}\n",
    "\n",
    "sweep_config['metric']= 'val_loss'\n",
    "\n",
    "# now name hyperparameters with nested dictionary\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "       'value':5\n",
    "    },\n",
    "    # for build_dataset\n",
    "     'batch_size': {\n",
    "       'distribution': 'int_uniform',  #we want to specify a distribution type to more efficiently iterate through these hyperparams\n",
    "       'min': 64,\n",
    "       'max': 128\n",
    "    },\n",
    "    'LSTM_layer_size_1': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_2': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_3': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_4': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_5': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "     'dropout': {\n",
    "             'distribution': 'uniform',\n",
    "       'min': 0,\n",
    "       'max': 0.6\n",
    "    },\n",
    "    'learning_rate':{\n",
    "         #uniform distribution between 0 and 1\n",
    "         'distribution': 'uniform', \n",
    "         'min': 0,\n",
    "         'max': 0.1\n",
    "     }\n",
    "}\n",
    "\n",
    "# append parameters to sweep config\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "# login to wandb-------------------------\n",
    "wandb.init(project=\"Thinking-Parrot2.0\", entity=\"oscarscholin\")\n",
    "\n",
    "# finish with callbacks------------\n",
    "# use the two helper functions above to create the LambdaCallback \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "# define two other callbacks\n",
    "# save model\n",
    "# if no directory \"models\" exists, create it\n",
    "if not(os.path.exists(os.path.join(MAIN_DIR, 'models'))):\n",
    "    os.mkdir(os.path.join(MAIN_DIR, 'models'))\n",
    "modelpath = os.path.join(MAIN_DIR, \"tp2_v0.0.1.hdf5\")\n",
    "checkpoint = ModelCheckpoint(modelpath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')\n",
    "# if learning stals, reduce the LR\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)\n",
    "\n",
    "# compile the callbacks\n",
    "callbacks = [print_callback, checkpoint, reduce_lr, WandbCallback()]\n",
    "# callbacks = [print_callback, checkpoint, reduce_lr]\n",
    "\n",
    "# initialize sweep!\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Thinking-Parrot2.0\", entity=\"oscarscholin\")\n",
    "\n",
    "# 'train' tells agent function is train\n",
    "# 'count': number of times to run this\n",
    "wandb.agent(sweep_id, train, count=100)\n",
    "\n",
    "#train_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14577, 200, 68)\n",
      "(3643, 200, 68)\n",
      "(14577, 68)\n",
      "(3643, 68)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaf8a3611b879056867134183afc22ea709e115b10fb7684e1dbf805b3500c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
