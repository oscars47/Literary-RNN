{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 09:34:29.579549: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-09 09:34:29.726521: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-09 09:34:29.747665: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:29.747671: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-09 09:34:30.204444: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:30.204476: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:30.204479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 09:34:31.197164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-09 09:34:31.197655: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:31.197688: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:31.197707: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:31.197728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:31.197748: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:31.197766: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:31.197784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:31.197804: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-09 09:34:31.197807: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'é', 'ê', 'ï', '—', '‘', '’', '“', '”']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moscarscholin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oscar47/Desktop/thinking_parrot/Literary-RNN/model_v0.1.0/wandb/run-20221209_093435-3dckwvok</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/3dckwvok\" target=\"_blank\">glorious-armadillo-2</a></strong> to <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Additional properties are not allowed ('goal' was unexpected)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. 'val_loss' is not of type 'object'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8slrn8h2\n",
      "Sweep URL: https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: Synced glorious-armadillo-2: https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/3dckwvok\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20221209_093435-3dckwvok/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: se21v0s3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_1: 122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_2: 84\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_3: 247\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_4: 127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_5: 77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 68\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3480846116282888\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.014238810008628444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oscar47/Desktop/thinking_parrot/Literary-RNN/model_v0.1.0/wandb/run-20221209_093445-se21v0s3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/se21v0s3\" target=\"_blank\">comic-sweep-1</a></strong> to <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2\" target=\"_blank\">https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 09:34:47.628898: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7851/7851 [==============================] - ETA: 0s - loss: 3.1065\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.1\n",
      "----- Generating with seed: \"t to times in hope, my verse shall stand.    Praising thy worth, despite his cruel handLXIIs it thy will, thy image should keep openMy heavy eyelids to the weary night?Dost \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">comic-sweep-1</strong>: <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/se21v0s3\" target=\"_blank\">https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/se21v0s3</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221209_093445-se21v0s3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run se21v0s3 errored: NameError(\"name 'char_to_int' is not defined\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 32wbwerj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_1: 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_2: 132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_3: 190\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_4: 107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_5: 247\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.21731896205775303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0010452349502798608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oscar47/Desktop/thinking_parrot/Literary-RNN/model_v0.1.0/wandb/run-20221209_103802-32wbwerj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/32wbwerj\" target=\"_blank\">easy-sweep-2</a></strong> to <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2\" target=\"_blank\">https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7519/7519 [==============================] - ETA: 0s - loss: 3.0902\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.1\n",
      "----- Generating with seed: \"ure spirit in her face.I cannot tell why each revolving seasonEnhanced hr beauty thus. Some say the reasonWas in the stars; _I_ think those lum\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84f12995f5a49faa1a39a0303d7db65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">easy-sweep-2</strong>: <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/32wbwerj\" target=\"_blank\">https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/32wbwerj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221209_103802-32wbwerj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 32wbwerj errored: NameError(\"name 'char_to_int' is not defined\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e2l9qbu3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_1: 132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_2: 222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_3: 194\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_4: 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_5: 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 94\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4869387925945962\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0047892043742785665\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oscar47/Desktop/thinking_parrot/Literary-RNN/model_v0.1.0/wandb/run-20221209_115350-e2l9qbu3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/e2l9qbu3\" target=\"_blank\">solar-sweep-3</a></strong> to <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2\" target=\"_blank\">https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5680/5680 [==============================] - ETA: 0s - loss: 3.0912\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.1\n",
      "----- Generating with seed: \"e sweets and beauties do themslves forsakeAnd die as fast a\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8516d3ea8414a1bbc1d9ec315c5283c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-sweep-3</strong>: <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/e2l9qbu3\" target=\"_blank\">https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/e2l9qbu3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221209_115350-e2l9qbu3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run e2l9qbu3 errored: NameError(\"name 'char_to_int' is not defined\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n67dhikk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_1: 104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_2: 102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_3: 164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_4: 109\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layer_size_5: 70\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 66\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3909842748666507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.06940136114378313\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oscar47/Desktop/thinking_parrot/Literary-RNN/model_v0.1.0/wandb/run-20221209_131453-n67dhikk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/runs/n67dhikk\" target=\"_blank\">logical-sweep-4</a></strong> to <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2\" target=\"_blank\">https://wandb.ai/oscarscholin/Thinking-Parrot2.0/sweeps/8slrn8h2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# file to train network\n",
    "# @oscars47\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import *\n",
    "\n",
    "# check GPU num\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from dataprep2 import TextData # import TextData class for processing\n",
    "from modelpredict2 import * # get functions to interpret output\n",
    "\n",
    "# define path\n",
    "MAIN_DIR = '/home/oscar47/Desktop/thinking_parrot'\n",
    "DATA_DIR = os.path.join(MAIN_DIR, 'texts_prep')\n",
    "\n",
    "# define master txt file\n",
    "MASTER_TEXT_PATH = os.path.join(MAIN_DIR, 'texts', 'master.txt')\n",
    "\n",
    "# initialize text object\n",
    "maxChar = 100\n",
    "master=TextData(MASTER_TEXT_PATH, maxChar)\n",
    "# get alphabet\n",
    "alphabet = master.alphabet\n",
    "char_to_int= master.char_to_int\n",
    "int_to_char = master.int_to_char\n",
    "text = master.text\n",
    "\n",
    "# read in files for training\n",
    "x_train = np.load(os.path.join(DATA_DIR, 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(DATA_DIR, 'y_train.npy'))\n",
    "x_val = np.load(os.path.join(DATA_DIR, 'x_val.npy'))\n",
    "y_val = np.load(os.path.join(DATA_DIR, 'y_val.npy'))\n",
    "\n",
    "# build model functions--------------------------------\n",
    "def build_model(LSTM_layer_size_1,  LSTM_layer_size_2, LSTM_layer_size_3, \n",
    "          LSTM_layer_size_4, LSTM_layer_size_5, \n",
    "          dropout, learning_rate):\n",
    "    # call initialize function\n",
    "    \n",
    "    model = Sequential()\n",
    "    # RNN layers for language processing\n",
    "    model.add(LSTM(LSTM_layer_size_1, input_shape = (2*maxChar, len(alphabet)), return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_2, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_3, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_4, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_5))\n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(len(alphabet)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "    # put structure together\n",
    "    optimizer = RMSprop(learning_rate = learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "    # If called by wandb.agent, as below,\n",
    "    # this config will be set by Sweep Controller\n",
    "      config = wandb.config\n",
    "\n",
    "      #pprint.pprint(config)\n",
    "\n",
    "      #initialize the neural net; \n",
    "      global model\n",
    "      model = build_model(config.LSTM_layer_size_1,  config.LSTM_layer_size_2, config.LSTM_layer_size_3, \n",
    "              config.LSTM_layer_size_4, config.LSTM_layer_size_5, \n",
    "              config.dropout, config.learning_rate)\n",
    "      \n",
    "      #now run training\n",
    "      history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = config.batch_size,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=config.epochs,\n",
    "        callbacks=callbacks #use callbacks to have w&b log stats; will automatically save best model                     \n",
    "      ) \n",
    "\n",
    "# helper functions from Keras\n",
    "\n",
    "# do this each time we begin a new epoch    \n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = np.random.randint(1, len(text) - maxChar - 1)\n",
    "    # need to check how much to pad\n",
    "    if start_index < maxChar:\n",
    "        sentence0 = text[0:start_index]\n",
    "        sentence1 = text[start_index+1: start_index+start_index]\n",
    "        sentence = sentence0+sentence1\n",
    "    else:\n",
    "        stdev = (1/2)*(maxChar - 1)\n",
    "        mean = (maxChar - 1)\n",
    "        toast_len = int(np.random.normal(mean, stdev)) # get normalized-skewed toast length\n",
    "        sentence0 = text[start_index-toast_len:start_index]\n",
    "        sentence1 = text[start_index+1: start_index+toast_len]\n",
    "        sentence =  sentence0+ sentence1\n",
    "    \n",
    "\n",
    "    # 1. compute difference from maxChar and len/2\n",
    "    diff = maxChar - int(len(sentence)/2)\n",
    "    # 2. initialize new string for each sentence\n",
    "    complete_sentence = ''\n",
    "    for i in range(diff):\n",
    "        complete_sentence+='£' # appending forbidden\n",
    "    # 3. now add 'real' sentence\n",
    "    complete_sentence+=sentence\n",
    "    # 4. append forbidden again\n",
    "    for i in range(diff):\n",
    "        complete_sentence+='£'\n",
    "\n",
    "\n",
    "    for diversity in [0.1, 0.2, 0.5, 1.0, 1.2, 1.5, 2.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        #generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        #sys.stdout.write(generated)\n",
    "\n",
    "        # generate 400 characters worth of test\n",
    "        for i in range(400):\n",
    "            # prepare chosen sentence as part of new dataset\n",
    "            x_pred = np.zeros((1, maxChar, len(alphabet)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                if char != '£': # encode 1 iff it's not padded\n",
    "                    x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "            # use the current model to predict what outputs are\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            # call the function above to interpret the probabilities and add a degree of freedom\n",
    "            next_index = sample(preds, diversity)\n",
    "            #convert predicted number to character\n",
    "            next_char = int_to_char[next_index]\n",
    "\n",
    "            generated+=next_char\n",
    "\n",
    "            # check size of sentence; if still small can keep old stuff in sentence0\n",
    "            if len(sentence) >= 2*len(maxChar):\n",
    "                sentence0 = sentence0[1:]\n",
    "            sentence0 += next_char # append new middle character\n",
    "            sentence=sentence0+sentence1 # append to main sentence\n",
    "\n",
    "            # print the new character as we create it\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "# define search parameters-----------------\n",
    "# holds wandb config nested dictionaries\n",
    "# @oscars47\n",
    "\n",
    "# set dictionary with random search; optimizing val_loss\n",
    "sweep_config= {\n",
    "    'method': 'random',\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'\n",
    "}\n",
    "\n",
    "sweep_config['metric']= 'val_loss'\n",
    "\n",
    "# now name hyperparameters with nested dictionary\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "       'value':5\n",
    "    },\n",
    "    # for build_dataset\n",
    "     'batch_size': {\n",
    "       'distribution': 'int_uniform',  #we want to specify a distribution type to more efficiently iterate through these hyperparams\n",
    "       'min': 64,\n",
    "       'max': 128\n",
    "    },\n",
    "    'LSTM_layer_size_1': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_2': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_3': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_4': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_5': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "     'dropout': {\n",
    "             'distribution': 'uniform',\n",
    "       'min': 0,\n",
    "       'max': 0.6\n",
    "    },\n",
    "    'learning_rate':{\n",
    "         #uniform distribution between 0 and 1\n",
    "         'distribution': 'uniform', \n",
    "         'min': 0,\n",
    "         'max': 0.1\n",
    "     }\n",
    "}\n",
    "\n",
    "# append parameters to sweep config\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "# login to wandb-------------------------\n",
    "wandb.init(project=\"Thinking-Parrot2.0\", entity=\"oscarscholin\")\n",
    "\n",
    "# finish with callbacks------------\n",
    "# use the two helper functions above to create the LambdaCallback \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "# define two other callbacks\n",
    "# save model\n",
    "# if no directory \"models\" exists, create it\n",
    "if not(os.path.exists('models')):\n",
    "    os.mkdir('./models/')\n",
    "modelpath = \"models/shakespeare_v0.0.1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(modelpath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')\n",
    "# if learning stals, reduce the LR\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)\n",
    "\n",
    "# compile the callbacks\n",
    "callbacks = [print_callback, checkpoint, reduce_lr, WandbCallback()]\n",
    "\n",
    "# initialize sweep!\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Thinking-Parrot2.0\", entity=\"oscarscholin\")\n",
    "\n",
    "# 'train' tells agent function is train\n",
    "# 'count': number of times to run this\n",
    "wandb.agent(sweep_id, train, count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaf8a3611b879056867134183afc22ea709e115b10fb7684e1dbf805b3500c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
