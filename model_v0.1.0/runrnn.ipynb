{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "[' ', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’']\n",
      "456/456 [==============================] - ETA: 0s - loss: 3.1208\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.1\n",
      "199\n",
      "189\n",
      "----- Generating with seed: \"esurface of the bread before placing into the toaster. This will help prevent piecesfrom fallin into the toaster, sticking on the guard wire in the slot or burninginside the toaster.8. Unev\"\n",
      "  ae erc t   ele o5s   ee tano  hh  n .    od   e  Raeano     te       nnragg   c   oe. bndaoo ec  dan    a I tt . ioie o tt  etr. t tdeeeth b    la  eo fe hp oa etnnacdg a ee  feab     e   uea tde    e   an  d ieee    eh  e o e pg dne  oen din a ile  g D eso ooseepi   o  a  i thet o2dendn eet     ta s h  g en io  t  eUeeC     dtehoi nnd eehg   en r  tfhh  e eiehae  b ae aodhe    ii       tee  ac \n",
      "----- diversity: 0.5\n",
      "199\n",
      "27\n",
      "----- Generating with seed: \"ry laws, (excldingthe Austr\"\n",
      "a s in echr  iea  ea o aw t  eca e e f  t eteet   o rde tse  g e a eeeceiaa oa e sa    r  De e teidnoeRat v      f  nelec  ,   i dinsTt f  tiaIti  ta e otoaefe d  emnes ic  Ti    a   ar  ieehn hl tet e   uen o eae  nrseo e inr i o g  e  neeaa   t  diaa rebd a a sree ue  eai Oh e  t imte eneb bglsd f o   eho  n iaar bt.  a isaL   niaon itad  na a hettc   be haohaerf  gn re ee W eo t  .n hetaiaanc l\n",
      "----- diversity: 1.2\n",
      "199\n",
      "99\n",
      "----- Generating with seed: \"the toaster is in use. Toasting is a combination o cooking and drying the bread.2. Moisture levels \"\n",
      "Da  taia aeant a iy e  l br  rf tt    ki  hae  t ldleeht  (  are a oT m e    l .  l cea aa  e l no i eheef e wr ci  f  a n  so  oth  ee aen   haneapn  h e ni c  a fh f   e os    iem   Wrai  ai  e e  eor  eet ad  d o      nNe .      baooR a  eo. e iO oeen  a hnonn o sieof n ota e    et    a o ar3en a  n  go  ate Th    ra  sn i ttheal       etahe fm dpntieemdpenddiaa r i tn ec  ton u e   e  i l sbe.\n",
      "\n",
      "Epoch 1: loss improved from inf to 3.12084, saving model to /home/oscar47/Desktop/thinking_parrot/tp2_v0.0.1.hdf5\n",
      "456/456 [==============================] - 197s 425ms/step - loss: 3.1208 - val_loss: 3.1812 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# file to train network\n",
    "# @oscars47\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import *\n",
    "\n",
    "# check GPU num\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from dataprep2 import TextData # import TextData class for processing\n",
    "from modelpredict2 import * # get functions to interpret output\n",
    "\n",
    "# define path\n",
    "MAIN_DIR = '/home/oscar47/Desktop/thinking_parrot'\n",
    "DATA_DIR = os.path.join(MAIN_DIR, 'texts_prep') # main\n",
    "# DATA_DIR = os.path.join(MAIN_DIR, 'texts_prep', 'test') # for testing\n",
    "\n",
    "# define master txt file\n",
    "MASTER_TEXT_PATH = os.path.join(MAIN_DIR, 'texts', 'master.txt')\n",
    "#MASTER_TEXT_PATH = os.path.join(MAIN_DIR, 'texts', 'toaster_man.txt')\n",
    "\n",
    "# initialize text object\n",
    "maxChar = 100\n",
    "master=TextData(MASTER_TEXT_PATH, maxChar)\n",
    "# get alphabet\n",
    "alphabet = master.alphabet\n",
    "char_to_int= master.char_to_int\n",
    "int_to_char = master.int_to_char\n",
    "text = master.text\n",
    "\n",
    "# read in files for training\n",
    "x_train = np.load(os.path.join(DATA_DIR, 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(DATA_DIR, 'y_train.npy'))\n",
    "x_val = np.load(os.path.join(DATA_DIR, 'x_val.npy'))\n",
    "y_val = np.load(os.path.join(DATA_DIR, 'y_val.npy'))\n",
    "\n",
    "# build model functions--------------------------------\n",
    "def build_model(LSTM_layer_size_1,  LSTM_layer_size_2, LSTM_layer_size_3, \n",
    "          LSTM_layer_size_4, LSTM_layer_size_5, \n",
    "          dropout, learning_rate):\n",
    "    # call initialize function\n",
    "    \n",
    "    model = Sequential()\n",
    "    # RNN layers for language processing\n",
    "    model.add(LSTM(LSTM_layer_size_1, input_shape = (2*maxChar, len(alphabet)), return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_2, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_3, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_4, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_5))\n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(len(alphabet)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "    # put structure together\n",
    "    optimizer = RMSprop(learning_rate = learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "    # If called by wandb.agent, as below,\n",
    "    # this config will be set by Sweep Controller\n",
    "      config = wandb.config\n",
    "\n",
    "      #pprint.pprint(config)\n",
    "\n",
    "      #initialize the neural net; \n",
    "      global model\n",
    "      model = build_model(config.LSTM_layer_size_1,  config.LSTM_layer_size_2, config.LSTM_layer_size_3, \n",
    "              config.LSTM_layer_size_4, config.LSTM_layer_size_5, \n",
    "              config.dropout, config.learning_rate)\n",
    "      \n",
    "      #now run training\n",
    "      history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size = config.batch_size,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=config.epochs,\n",
    "        callbacks=callbacks #use callbacks to have w&b log stats; will automatically save best model                     \n",
    "      ) \n",
    "\n",
    "def train_custom(LSTM_layer_size_1=128,  LSTM_layer_size_2=128, LSTM_layer_size_3=128, \n",
    "              LSTM_layer_size_4=128, LSTM_layer_size_5=128, \n",
    "              dropout=0.1, learning_rate=0.01, epochs=1, batchsize=32):\n",
    "    #initialize the neural net; \n",
    "    global model\n",
    "    model = build_model(LSTM_layer_size_1,  LSTM_layer_size_2, LSTM_layer_size_3, \n",
    "            LSTM_layer_size_4, LSTM_layer_size_5, \n",
    "            dropout, learning_rate)\n",
    "    \n",
    "    #now run training\n",
    "    history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = batchsize,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks #use callbacks to have w&b log stats; will automatically save best model                     \n",
    "    ) \n",
    "\n",
    "\n",
    "# helper functions from Keras\n",
    "\n",
    "def get_toast_len(mean, stdev):\n",
    "    toast_len = int(np.random.normal(mean, stdev))\n",
    "    return toast_len\n",
    "\n",
    "# do this each time we begin a new epoch    \n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "\n",
    "    for diversity in [0.1, 0.5,1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        start_index = np.random.randint(0, len(text) - maxChar - 1) +1\n",
    "        # need to check how much to pad\n",
    "        if start_index < maxChar:\n",
    "            sentence0 = text[0:start_index] # up to but not including start index\n",
    "            sentence1 = text[start_index+1: start_index+start_index+1]\n",
    "            sentence = sentence0+sentence1\n",
    "        else:\n",
    "            stdev = (1/2)*(maxChar - 1)\n",
    "            mean = (maxChar - 1)\n",
    "                 # compute len, following normal distribution between 1 and maxChar; will go from [:num] as first part then [num+1:] concatenated; predict at num\n",
    "                # need toastlen positive but no more than  maxChar\n",
    "            goodtoast = False\n",
    "            while goodtoast==False:\n",
    "                toast_len = get_toast_len(mean, stdev)\n",
    "                # add 1 to len since the distr here goes from 0 up to maxChar-1\n",
    "                toast_len+=1\n",
    "                #print(toast_len)\n",
    "                if (toast_len > 0) and (toast_len <= maxChar): # if get acceptable toast, can leave\n",
    "                    goodtoast=False\n",
    "                    break\n",
    "            sentence0 = text[start_index-toast_len:start_index]\n",
    "            sentence1 = text[start_index+1: start_index+toast_len]\n",
    "            sentence =  sentence0+ sentence1\n",
    "        \n",
    "        # need another condition here about if neat the end\n",
    "\n",
    "        # 1. compute difference from maxChar and len/2\n",
    "        diff = maxChar - int(len(sentence)/2)\n",
    "        # need to check even/odd so we don;t overcount\n",
    "        if len(sentence) %2 != 0: # if odd: need to subtract 1\n",
    "            diff-=1\n",
    "\n",
    "        # 2. initialize new string for each sentence\n",
    "        complete_sentence = ''\n",
    "        for i in range(diff):\n",
    "            complete_sentence+='£' # appending forbidden\n",
    "        # 3. now add 'real' sentence\n",
    "        complete_sentence+=sentence\n",
    "        # 4. append forbidden again\n",
    "        for i in range(diff):\n",
    "            complete_sentence+='£'\n",
    "\n",
    "        print(len(complete_sentence))\n",
    "        print(len(sentence))\n",
    "\n",
    "        generated = ''\n",
    "        #generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        #sys.stdout.write(generated)\n",
    "\n",
    "        # generate 400 characters worth of test\n",
    "        for i in range(400):\n",
    "            # prepare chosen sentence as part of new dataset\n",
    "            x_pred = np.zeros((1, 2*maxChar, len(alphabet)))\n",
    "            #x_pred = np.zeros((2*maxChar, len(alphabet)))\n",
    "            for t, char in enumerate(complete_sentence):\n",
    "                if char != '£': # encode 1 iff it's not padded\n",
    "                    x_pred[0, t, char_to_int[char]] = 1.\n",
    "                    #x_pred[t, char_to_int[char]] = 1.\n",
    "\n",
    "            # use the current model to predict what outputs are\n",
    "            preds = model.predict(x_pred, verbose=0)[0] # removed [0] here\n",
    "            # call the function above to interpret the probabilities and add a degree of freedom\n",
    "            next_index = sample(preds, diversity)\n",
    "            #convert predicted number to character\n",
    "            next_char = int_to_char[next_index]\n",
    "\n",
    "            generated+=next_char\n",
    "\n",
    "            # check size of sentence; if still small can keep old stuff in sentence0\n",
    "            if len(sentence) >= 2*maxChar:\n",
    "                sentence0 = sentence0[1:]\n",
    "            sentence0 += next_char # append new middle character\n",
    "            sentence=sentence0+sentence1 # append to main sentence\n",
    "\n",
    "            # print the new character as we create it\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "# define search parameters-----------------\n",
    "# holds wandb config nested dictionaries\n",
    "# @oscars47\n",
    "\n",
    "# set dictionary with random search; optimizing val_loss\n",
    "sweep_config= {\n",
    "    'method': 'random',\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'\n",
    "}\n",
    "\n",
    "sweep_config['metric']= 'val_loss'\n",
    "\n",
    "# now name hyperparameters with nested dictionary\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "       'value':5\n",
    "    },\n",
    "    # for build_dataset\n",
    "     'batch_size': {\n",
    "       'distribution': 'int_uniform',  #we want to specify a distribution type to more efficiently iterate through these hyperparams\n",
    "       'min': 64,\n",
    "       'max': 128\n",
    "    },\n",
    "    'LSTM_layer_size_1': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_2': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_3': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_4': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "    'LSTM_layer_size_5': {\n",
    "       'distribution': 'int_uniform',\n",
    "       'min': 64,\n",
    "       'max': 256\n",
    "    },\n",
    "     'dropout': {\n",
    "             'distribution': 'uniform',\n",
    "       'min': 0,\n",
    "       'max': 0.6\n",
    "    },\n",
    "    'learning_rate':{\n",
    "         #uniform distribution between 0 and 1\n",
    "         'distribution': 'uniform', \n",
    "         'min': 0,\n",
    "         'max': 0.1\n",
    "     }\n",
    "}\n",
    "\n",
    "# append parameters to sweep config\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "# login to wandb-------------------------\n",
    "#wandb.init(project=\"Thinking-Parrot2.0\", entity=\"oscarscholin\")\n",
    "\n",
    "# finish with callbacks------------\n",
    "# use the two helper functions above to create the LambdaCallback \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "# define two other callbacks\n",
    "# save model\n",
    "# if no directory \"models\" exists, create it\n",
    "if not(os.path.exists(os.path.join(MAIN_DIR, 'models'))):\n",
    "    os.mkdir(os.path.join(MAIN_DIR, 'models'))\n",
    "modelpath = os.path.join(MAIN_DIR, \"tp2_v0.0.1.hdf5\")\n",
    "checkpoint = ModelCheckpoint(modelpath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')\n",
    "# if learning stals, reduce the LR\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)\n",
    "\n",
    "# compile the callbacks\n",
    "callbacks = [print_callback, checkpoint, reduce_lr, WandbCallback()]\n",
    "# callbacks = [print_callback, checkpoint, reduce_lr]\n",
    "\n",
    "# initialize sweep!\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Thinking-Parrot2.0\", entity=\"oscarscholin\")\n",
    "\n",
    "# 'train' tells agent function is train\n",
    "# 'count': number of times to run this\n",
    "wandb.agent(sweep_id, train, count=100)\n",
    "\n",
    "#train_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14577, 200, 68)\n",
      "(3643, 200, 68)\n",
      "(14577, 68)\n",
      "(3643, 68)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaf8a3611b879056867134183afc22ea709e115b10fb7684e1dbf805b3500c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
